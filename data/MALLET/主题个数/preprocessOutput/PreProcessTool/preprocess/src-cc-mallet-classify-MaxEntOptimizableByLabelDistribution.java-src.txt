classify io serializable iterator logging logger optimize memory f g optimize optimizable type alphabet type feature selection type feature vector type instance type instance type label alphabet type label type matrix ops logger progress message logger math max ent optimizable label optimizable gradient value serializable t o o need done? logger logger logger get logger max ent optimizable label get name logger progress logger progress message logger get logger max ent optimizable label get name + pl xxx why test maximizable fail variance very small? e f u l t g u n p r o r v r n e 1 e f u l t g u n p r o r v r n e 1 0 e f u l t m m z e r l memory f g gaussian prior variance e f u l t g u n p r o r v r n e maximiz e f u l t m m z e r l constraint cache gradient max ent classifier instance training expectation temporarily store cache gradient cache value cache value stale cache gradient stale num label num feature feature index just clarity feature selection feature selection feature selection label feature selection num get value call 0 num get value gradient call 0 max ent optimizable label max ent optimizable label instance training max ent classifier training training alphabet fd training get alphabet label alphabet ld label alphabet training get target alphabet t fd stop growth because someone want feature induction ld stop growth add feature feature num label ld size num feature fd size + 1 feature index num feature 1 num label num feature constraint num label num feature cache gradient num label num feature fill 0 0 fill constraint 0 0 fill cache gradient 0 0 feature selection training get feature selection label feature selection training get label feature selection add feature index selection feature selection feature selection add feature index label feature selection 0 label feature selection length i++ label feature selection add feature index xxx late change both select flag? feature selection || label feature selection classifier classifier classifier classifier feature selection classifier feature selection label feature selection classifier feature selection feature index classifier feature index classifier get instance pipe training get pipe classifier classifier max ent training get pipe feature selection label feature selection cache value stale cache gradient stale initialize constraint logger fine instance training + training size instance inst training instance weight training get instance weight inst label label inst get label label logger fine instance +ii+ label +label feature vector fv feature vector inst get alphabet fdict fv get alphabet fv get alphabet fd here difference between single label rath picking best index loop over label index label num location training get target alphabet size po 0 po label num location pos++ matrix ops row plus equal constraint num feature label index location po fv instance weight label value location po na n instance weight instance weight na n na n 0 fv num location i++ na n fv value location logger info na n feature + fdict lookup fv index location na n na n logger info na n instance + inst get name feature weight 1 0 po 0 po label num location pos++ constraint label index location po num feature + feature index + 1 0 instance weight label value label index location po max ent get classifier classifier get parameter index index parameter index v cache value stale cache gradient stale index v get num length get buff buff || buff length length buff length arraycopy 0 buff 0 length buff buff cache value stale cache gradient stale buff length length buff length arraycopy buff 0 0 buff length log probability training label distribution get value cache value stale num get value calls++ cache value 0 we ll store expectation value cache gradient now cache gradient stale matrix ops cache gradient 0 0 incorporate likelihood score training get target alphabet size value 0 0 iterator instance it training iterator 0 it next ii++ instance instance it next instance weight training get instance weight instance label label instance get label label l now +input alphabet size + regular feature classifier get classification score instance score feature vector fv feature vector instance get value 0 0 po 0 po label num location pos++ loop limin yao ll label index location po score ll 0 label value location po 0 logger warning instance +instance get + infinite value skip value gradient cache value n e g t v e n f n t y cache value stale cache value label value location po 0 value instance weight label value location po math log score ll na n value logger fine max ent optimizable label instance + instance get name + na n value infinite value logger warning instance +instance get + infinite value skip value gradient cache value value cache value stale value cache value + value model expectation? limin yao si 0 si score length si++ score si 0 infinite score si matrix ops row plus equal cache gradient num feature si fv instance weight score si cache gradient num feature si + feature index + instance weight score si logger info expectation cache gradient print incorporate prior prior 0 0 num label li++ fi 0 fi num feature fi++ num feature + fi prior + 2 gaussian prior variance o value cache value cache value + prior cache value 1 0 m m z e n o t m n m z e cache value stale progress logger info value label prob + o value + prior + prior + loglikelihood +cach value cache value get value gradient buffer gradient constraint expectation gaussian prior variance cache gradient stale num get value gradient calls++ cache value stale fill cache gradient expectation get value matrix ops plus equal cache gradient constraint incorporate prior matrix ops plus equal cache gradient 1 0 gaussian prior variance parameter infinity external user we gradient 0 because parameter value can nev change anyway mess up future calculation matrix norm matrix ops substitute cache gradient n e g t v e n f n t y 0 0 zero gradient dimension among select feature label feature selection label index 0 label index num label label index++ matrix ops row cache gradient num feature label index 0 0 feature selection label index 0 label index num label label index++ matrix ops row cache gradient num feature label index 0 0 label feature selection label index cache gradient stale buffer buffer length length arraycopy cache gradient 0 buffer 0 cache gradient length max ent trainer gradient infinity norm + matrix ops infinity norm cache gradient these really public? why? count many time trainer compute gradient log probability training label get value gradient call num get value gradient call count many time trainer compute log probability training label get value call num get value call get iteration maximiz gradient get iteration max ent optimizable label gaussian prior set parameter prevent overtrain small variance prior feature weight expect hover closer 0 extra evidence high weight trainer max ent optimizable label gaussian prior variance gaussian prior variance gaussian prior variance gaussian prior variance 