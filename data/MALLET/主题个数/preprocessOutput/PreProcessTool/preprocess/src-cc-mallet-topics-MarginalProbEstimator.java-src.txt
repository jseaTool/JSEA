2005 univ dept part m l l e t m achine learning languag e toolkit 1 0 information see ` l e n e topic zip io format type random topic model marginal probability estimator present wallach et al evaluation topic model m l 2009 david mimno marginal prob estimator serializable num topic topic fit these value encode type topic count count topic pair single topic mask topic bit alpha dirichlet alpha alpha over topic alpha sum beta prior topic multinomial over word beta sum smooth mass 0 0 cache coefficient type topic count index feature index topic index topic index topic index random random print word probability marginal prob estimator num topic alpha alpha sum beta type topic count topic num topic num topic bit count num topic 1 exact power 2 topic mask num topic 1 topic bit bit count topic mask add extra bit topic mask high bit num topic 2 1 topic bit bit count topic mask type topic count type topic count topic topic alpha sum alpha sum alpha alpha beta beta beta sum beta type topic count length random random cache coefficient num topic initialize smooth sampling bucket smooth mass 0 initialize cache coefficient smooth these value selectively replace document zero count topic topic 0 topic num topic topic++ smooth mass + alpha topic beta topic topic + beta sum cache coefficient topic alpha topic topic topic + beta sum err topic evaluator + num topic + topic + topic bit + topic bit + binary topic mask + topic mask random seed seed seed 1 random random random random seed random random r random r get topic topic get type topic count type topic count print word print print word probability print evaluate left instance testing num particle resampl print stream doc probability stream log num particle math log num particle total log likelihood 0 instance instance testing feature feature instance get doc log likelihood 0 particle probability num particle particle 0 particle num particle particle++ particle probability particle left resampl position 0 position particle probability 0 length position++ sum 0 particle 0 particle num particle particle++ sum + particle probability particle position sum 0 0 log prob math log sum log num particle doc log likelihood + log prob print word probability word instance get alphabet lookup get index position position printf % %f word log prob doc probability stream doc probability stream doc log likelihood total log likelihood + doc log likelihood total log likelihood left feature resampl doc topic get length word probability get length current type topic count type old topic topic topic weight sum doc length get length keep track we ve examine vocabulary word far 0 local topic count num topic local topic index num topic build densely list topic zero count dense index 0 record total zero topic zero topic dense index initialize topic count beta sampling bucket topic beta mass 0 0 topic term mass 0 0 topic term score num topic topic term index topic term value score log likelihood 0 count now zero we starting completely fresh iterate over position word document limit 0 limit doc length limit++ record marginal probability current limit sum over topic resampl iterate up current limit position 0 position limit position++ type get index position position old topic doc topic position check vocabulary word type type topic count length || type topic count type current type topic count type topic count type count topic normalize constant note we clamp estimate p w|t we n o t change smooth mass topic beta mass beta local topic count old topic topic old topic + beta sum decrement local doc topic count local topic count old topic maintain dense index we delete old topic local topic count old topic 0 first get dense location old topic dense index 0 we know there somewhere we t need bound check local topic index dense index old topic dense index++ shift remain dense index left dense index zero topic dense index local topic index length 1 local topic index dense index local topic index dense index + 1 dense index++ zero topic add old topic back into normalize constant topic beta mass + beta local topic count old topic topic old topic + beta sum reset cache coefficient topic cache coefficient old topic alpha old topic + local topic count old topic topic old topic + beta sum now go over type topic count calculate score topic index 0 current topic current value already decrement topic term mass 0 0 index current type topic count length current type topic count index 0 current topic current type topic count index topic mask current value current type topic count index topic bit score cache coefficient current topic current value topic term mass + score topic term score index score index++ sample random next uniform smooth mass + topic beta mass + topic term mass orig sample sample sure actually get topic 1 sample topic term mass 1 sample 0 i++ sample topic term score topic current type topic count topic mask sample topic term mass sample topic beta mass beta topic count++ sample beta dense index 0 dense index zero topic dense index++ topic local topic index dense index sample local topic count topic topic topic + beta sum sample 0 0 topic topic smooth count++ sample topic beta mass sample beta topic 0 sample alpha topic topic topic + beta sum sample 0 0 topic++ sample alpha topic topic topic + beta sum topic 1 err sampling + orig sample + + sample + + smooth mass + + topic beta mass + + topic term mass topic num topic 1 t o o appropriate illegal worker runnable topic sample topic 1 put topic into count doc topic position topic topic beta mass beta local topic count topic topic topic + beta sum local topic count topic ++ topic document add topic dense index local topic count topic 1 first find point we insert topic going reason we re keeping track zero topic working backward dense index zero topic dense index 0 local topic index dense index 1 topic local topic index dense index local topic index dense index 1 dense index local topic index dense index topic zero topics++ update coefficient zero topic cache coefficient topic alpha topic + local topic count topic topic topic + beta sum topic beta mass + beta local topic count topic topic topic + beta sum we ve just resampl u p t o current limit now sample t current limit type get index position limit check vocabulary word type type topic count length || type topic count type current type topic count type topic count type index 0 current topic current value topic term mass 0 0 index current type topic count length current type topic count index 0 current topic current type topic count index topic mask current value current type topic count index topic bit score cache coefficient current topic current value topic term mass + score topic term score index score + current topic + + current value index++ debug sure we re getting probability topic 0 topic num topic topic++ index 0 count 0 index current type topic count length current type topic count index 0 current topic current type topic count index topic mask current value current type topic count index topic bit current topic topic count current value index++ print topic + print + local topic count topic + + + alpha topic + + + alpha sum + + + far + + count + + + beta + + + topic topic + + + beta sum + + count + beta topic topic + beta sum sample random next uniform smooth mass + topic beta mass + topic term mass orig sample sample note we ve be absorb alpha sum + doc length into normalize constant marginal probability need term we stick back word probability limit + smooth mass + topic beta mass + topic term mass alpha sum + far normalizer + alpha sum + + + far far++ sure actually get topic 1 sample topic term mass 1 sample 0 i++ sample topic term score topic current type topic count topic mask sample topic term mass sample topic beta mass beta topic count++ sample beta dense index 0 dense index zero topic dense index++ topic local topic index dense index sample local topic count topic topic topic + beta sum sample 0 0 topic topic smooth count++ sample topic beta mass sample beta topic 0 sample alpha topic topic topic + beta sum sample 0 0 topic++ sample alpha topic topic topic + beta sum topic 1 err sampling + orig sample + + sample + + smooth mass + + topic beta mass + + topic term mass topic num topic 1 t o o appropriate put topic into count doc topic limit topic topic beta mass beta local topic count topic topic topic + beta sum local topic count topic ++ topic document add topic dense index local topic count topic 1 first find point we insert topic going reason we re keeping track zero topic working backward dense index zero topic dense index 0 local topic index dense index 1 topic local topic index dense index local topic index dense index 1 dense index local topic index dense index topic zero topics++ update coefficient zero topic cache coefficient topic alpha topic + local topic count topic topic topic + beta sum topic beta mass + beta local topic count topic topic topic + beta sum type + + topic + + log likelihood clean up our mess reset coefficient value smooth next doc update zero topic dense index 0 dense index zero topic dense index++ topic local topic index dense index cache coefficient topic alpha topic topic topic + beta sum word probability serial u 1 u r r e n t e r l v e r o n 0 n u l l n t e g e r 1 write output stream o write u r r e n t e r l v e r o n write num topic write topic mask write topic bit write alpha write alpha sum write beta write beta sum write type topic count write topic write random write smooth mass write cache coefficient read input stream o found read num topic read topic mask read topic bit read alpha read alpha sum read beta read beta sum read type topic count read topic read random random read smooth mass read cache coefficient read marginal prob estimator read f marginal prob estimator estimator input stream ois input stream input stream f estimator marginal prob estimator ois read ois close estimator 