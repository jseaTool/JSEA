2002 univ dept part m l l e t m achine learning languag e toolkit 1 0 information see ` l e n e classify culotta cluster classify base classify io o io input stream io output stream iterator logging logger optimize conjugate gradient optimize memory f g optimize optimizable optimize optimiz type alphabet type gain type feature inducer type feature selection type feature vector type feature vector type gradient gain type info gain type instance type instance type label type label alphabet type label vector type label type matrix ops type rank feature vector command logger progress message logger math trainer link rank max ent classifier expect instance feature vector target representation index best feature vector note instance target label indicate tie best instance aron culotta culotta culotta rank max ent trainer max ent trainer logger logger logger get logger rank max ent trainer get name logger progress logger progress message logger get logger rank max ent trainer get name + pl rank max ent trainer construct trainer parameter overtrain 1 0 usually value rank max ent trainer gaussian prior variance gaussian prior variance optimizable gradient value get maximizable trainer instance ilist ilist maximizable trainer maximizable trainer ilist max ent train instance training logger fine training size +train size rank max ent trainer maximizable trainer mt rank max ent trainer maximizable trainer training rank max ent classifier optimiz maximiz memory f g mt maximiz optimize loop below seem wrong converge 0 num iteration i++ converge maximiz optimize 1 illegal argument e e print stack trace logger info catching saying converge converge converge num iteration m v l u e run again because our sam rowei experience f g can still eke likelihood first convergence re running being restrict gradient history optimiz conjugate gradient mt optimiz optimize illegal argument e e print stack trace logger info catching saying converge progress logger info proges message line move mt get classifier xxx won t work here fix p train feature induction some option change p maxent partially train classifier classifier dur training gain name estimate gain log likelihood increase we want our chosen feature maximize max ent trainer e p g n max ent trainer g r e n t g n max ent trainer n f o r m t o n g n e p g n train max ent classifier classifier train feature induction instance training instance validation instance testing classifier evaluate evaluator max ent maxent total iteration num iteration between feature induction num feature induction num feature feature induction gain name ought parameter setting can crash training jump too small save dur f alphabet input alphabet training get alphabet alphabet output alphabet training get target alphabet maxent maxent rank max ent training get pipe 1+input alphabet size output alphabet size training iteration 0 num label output alphabet size initialize feature selection feature selection global f training get feature selection global f mask feature some late feature inducer induce feature global f feature selection training get alphabet training feature selection global f validation validation feature selection global f testing testing feature selection global f maxent rank max ent maxent get instance pipe maxent get global f run feature induction feature induction iteration 0 feature induction iteration num feature induction feature induction iteration++ print some feature information logger info feature induction iteration +feature induction iteration train model little bit we t care converge we execute feature induction iteration matter what feature induction iteration 0 t train until we some feature num iteration num iteration between feature induction maxent rank max ent train training validation testing evaluator maxent training iteration + num iteration between feature induction logger info starting feature induction + 1+input alphabet size + feature over +num labels+ label instance instance instance training get alphabet training get target alphabet instance instance instance input alphabet output alphabet instance feature selection get examine feature inducer can know add singleton feature instance feature selection global f label vector these length 1 vector 0 training size i++ instance inst training get have train just current feature see we classify training now classification classification maxent classify inst classification best label correct instance il instance inst get instance sub instance il get inst get label get best label get entry value instance add sub instance label vector add classification get label vector label vector add label vector sub instance classification logger info instance size +error instance size label vector size label vector lv label vector 0 i++ lv label vector label vector get rank feature vector factory gain factory gain name equal e p g n gain factory gain factory lv gaussian prior variance gain name equal g r e n t g n gain factory gradient gain factory lv gain name equal n f o r m t o n g n gain factory info gain factory illegal argument unsupport gain name +gain name feature inducer klfi feature inducer gain factory instance num feature feature induction 2 num feature feature induction 2 num feature feature induction note add feature globally transition klfi induce feature training testing klfi induce feature testing logger info max ent feature selection now +global f cardinality + feature klfi 1+input alphabet size output alphabet size executing block often dur training t know why save dur f keep current parameter value relie detail most recent feature alphabet get high index count output label old count maxent length output alphabet size count 1+input alphabet size into proper location 0 output alphabet size i++ arraycopy maxent old count count old count 0 old count i++ maxent maxent + +new exit 0 maxent maxent feature index input alphabet size finish feature induction logger info end +global f cardinality + feature num iteration total iteration training iteration train training validation testing evaluator maxent rank max ent trainer + +maximiz get name + + num iteration + num iteration + gaussian prior variance +gaussian prior variance inner wrap up rank max ent classifier training maximize maximizable function maximizable trainer optimizable gradient value constraint cache gradient rank max ent classifier instance training expectation temporarily store cache gradient cache value cache value stale cache gradient stale num label num feature feature index just clarity feature selection feature selection feature selection label feature selection maximizable trainer maximizable trainer instance ilist rank max ent classifier training ilist alphabet fd ilist get alphabet label alphabet ld label alphabet ilist get target alphabet t fd stop growth because someone want feature induction ld stop growth add feature feature assume underlie instance binary num label underlie label alphabet size xxx num label 2 num feature fd size + 1 feature index num feature 1 num label num feature constraint num label num feature cache gradient num label num feature fill 0 0 fill constraint 0 0 fill cache gradient 0 0 feature selection ilist get feature selection label feature selection ilist get label feature selection add feature index selection feature selection feature selection add feature index label feature selection 0 label feature selection length i++ label feature selection add feature index xxx late change both select flag? feature selection || label feature selection classifier classifier classifier classifier feature selection classifier feature selection label feature selection classifier feature selection feature index classifier feature index classifier get instance pipe ilist get pipe classifier classifier rank max ent ilist get pipe feature selection label feature selection cache value stale cache gradient stale initialize constraint constraint positive instance iterator instance it training iterator logger fine instance training + training size it next instance instance it next instance weight training get instance weight instance feature vector fv feature vector instance get label best instance sub target instance get target label label target label label label target get 0 label label target positive index value label get best label get entry value positive index 1 instance logger warning label 1 skip feature vector fv feature vector fv get positive index alphabet fdict fv get alphabet fv get alphabet fd xxx ensure dimensionality constraint correct matrix ops row plus equal constraint num feature 0 fv instance weight feature weight 1 0 na n instance weight instance weight na n na n best index na n na n 0 fv num location i++ na n fv value location logger info na n feature + fdict lookup fv index location na n na n logger info na n instance + instance get name constraint positive instance xxx constraint 0 num feature + feature index + 1 0 instance weight test maximizable test value gradient current rank max ent get classifier classifier get parameter index index parameter index v cache value stale cache gradient stale index v get num length get buff buff || buff length length buff length arraycopy 0 buff 0 length buff buff cache value stale cache gradient stale buff length length buff length arraycopy buff 0 0 buff length log probability training label here probability positive being label get value cache value stale cache value 0 we ll store expectation value cache gradient now cache gradient stale matrix ops cache gradient 0 0 incorporate likelihood value 0 0 iterator instance it training iterator 0 it next ii++ instance instance it next feature vector fv feature vector instance get score store pr sub being positive instance score fv size instance weight training get instance weight instance label representation indicate feature vector sub positive proceed usual penalize score duplicate entry improve accuracy some expt target instance get target 1 target label value label target value 1 hack instance 0 fv size classifier get classification score instance score target label label label label target best position label size pi 0 pi label size pi++ best position pi value label get pi best position 0 classifier get classification score tie instance score best position value instance weight math log score na n value logger fine max ent trainer instance + instance get name + na n value log score + math log score + score + score + instance weight + instance weight infinite value logger warning instance +instance get + infinite value skip value gradient cache value value cache value stale value cache value + value positive score score si 0 si fv size si++ score si 0 infinite score si feature vector cfv feature vector fv get si matrix ops row plus equal cache gradient num feature 0 cfv instance weight score si cache gradient num feature 0 + feature index + instance weight score si incorporate prior 0 num label li++ fi 0 fi num feature fi++ num feature + fi cache value + 2 gaussian prior variance cache value 1 0 m m z e n o t m n m z e cache value stale progress logger info value loglikelihood +cach value cache value get value gradient buffer gradient constraint expectation gaussian prior variance cache gradient stale cache value stale fill cache gradient expectation get value matrix ops plus equal cache gradient constraint incorporate prior matrix ops plus equal cache gradient 1 0 gaussian prior variance parameter infinity external user we gradient 0 because parameter value can nev change anyway mess up future calculation matrix norm matrix ops substitute cache gradient n e g t v e n f n t y 0 0 zero gradient dimension among select feature label feature selection label index 0 label index num label label index++ matrix ops row cache gradient num feature label index 0 0 feature selection label index 0 label index num label label index++ matrix ops row cache gradient num feature label index 0 0 label feature selection label index cache gradient stale buffer buffer length length arraycopy cache gradient 0 buffer 0 cache gradient length e r l z t o n serial u 1 u r r e n t e r l v e r o n 1 write output stream o write write u r r e n t e r l v e r o n read input stream o found read read 