2002 univ dept part m l l e t m achine learning languag e toolkit 1 0 information see ` l e n e aron culotta culotta culotta numerical recipe p 385 lnsrch simple backtrack line search attempt accurately finding goal ensure back track line search position high value optimize logging r f optimize line optimiz optimize optimizable type matrix ops line search backtrack p385 numeric recipe back track line search line optimiz gradient logger logger logger get logger back track line search get name optimizable gradient value function back track line search optimizable gradient value optimizable function optimizable max iteration 100 stpmax 100 e p 3 0e 12 termination ab delta r e l t o l coordinate ab delta t o l coordinate function increase us l f rel tolx 1e 7 ab tolx 1e 4 tolerance absolute value difference l f 1e 4 set tolerance relative diff function value line search converge tt ab delta tolx tt coordinate rel tolx tolx rel tolx tolx set tolerance absolute diff function value line search converge tt ab delta tolx tt coordinate ab tolx tolx ab tolx tolx step ignore step 1 0 sometime confus backtrack reason t understand jump get l r g e r iteration 1 fraction step size alam found good step 0 0 could step direction optimize line step g old slope slope temp test alamin alam alam2 tmplam rhs1 rhs2 disc old alam f fold f2 g function get num gradient function get num old function get num function get arraycopy 0 old 0 length function get value gradient g alam2 tmplam 0 0 f2 fold function get value logger loggable level f n e logger fine e n t e r n g k t r k logger fine entering back track ln srch value +fold+ direction norm + matrix ops norm line + direction inf norm + matrix ops infinity norm line matrix ops na n g sum matrix ops two norm line sum stpmax logger warning attempt step too big scaling sum +sum+ stpmax +stpmax matrix ops time equal line stpmax sum slope slope matrix ops dot g line logger fine slope +slope slope 0 optimizable slope + slope + negative slope 0 optimizable slope + slope + zero find maximum lambda converge delta r e l t o l coordinate large step size trigger threshold precomput save alamin test 0 0 0 old length i++ temp math ab line math max math ab old 1 0 temp test test temp alamin rel tolx test alam 1 0 old alam 0 0 iteration 0 look step size direction line iteration 0 iteration max iteration iteration++ old + alam line initially alam 1 0 e take full newton step logger fine back track loop iteration +iteration+ alam + alam+ old alam +old alam logger fine step 1norm + matrix ops norm + alam + alam + old alam + old alam alam old alam alam old alam matrix ops plus equal line alam old alam step logger fine step 1norm + matrix ops norm check convergence convergence delta alam alamin || small ab diff old alam alamin function old f function get value logger warning e t n g k t r k jump too small alamin +alamin+ exit xold value +f 0 0 function old alam alam f function get value logger fine value +f function increase wolf f fold+ l f alam slope logger fine e t n g k t r k value +f f fold illegal function increase f + f + + fold + fold alam value infinite e we ve jump unstable territory scale down jump infinite f || infinite f2 logger warning value infinite jump + old alam + f +f+ f2 +f2+ scaling back step size tmplam 2 alam alam alamin convergence delta function old f function get value logger warning e t n g k t r k jump too small exit xold value +f 0 0 backtrack alam 1 0 first tmplam slope 2 0 f fold slope rhs1 f fold alam slope rhs2 f2 fold alam2 slope alam alam2 0 f l u r e divide alam alam2 alam +alam rhs1 alam alam rhs2 alam2 alam2 alam alam2 alam2 rhs1 alam alam +alam rhs2 alam2 alam2 alam alam2 0 0 tmplam slope 2 0 disc 3 0 slope disc 0 0 tmplam 5 alam 0 0 tmplam b+ math sqrt disc 3 0 tmplam slope b+ math sqrt disc tmplam 5 alam tmplam 5 alam lambda 5 lambda 1 alam2 alam f2 f logger fine tmplam +tmplam alam math max tmplam 1 alam lambda 1 lambda 1 iteration max iteration illegal too many iteration 0 0 iff we ve converge absolute difference small ab diff xold 0 length i++ math ab xold ab tolx 