2002 univ dept part m l l e t m achine learning languag e toolkit 1 0 information see ` l e n e aron culotta culotta culotta memory f g describe byrd nocedal schnabel representation quasi newton matrix memory optimize logging link optimize back track line search optimize line optimiz optimize optimizable type matrix ops logger memory f g optimiz logger logger logger get logger base ml maximize memory f g converge optimizable gradient value optimizable max iteration 1000 xxx need principl stopping point tolerance 0001 tolerance 0001 gradient tolerance 001 ep 1 0e 5 correction f g update ideally 3 m 7 large m cpu memory m 4 line search function line optimiz gradient line maximiz memory f g optimizable gradient value function optimizable function line maximiz back track line search function optimizable get optimizable optimizable converge converge set line optimiz gradient l f g optimization line opt line optimiz l f g line optimiz line optimiz gradient line opt line maximiz line opt search g gradient m previou value y m previou g value rho intermediate calculation g oldg direction old link link link y link link rho link alpha step 1 0 iteration optimiz evaluator gradient eval p l tolerance newtol tolerance newtol evaluator optimiz evaluator gradient eval eval eval get iteration iteration optimize optimize m v l u e optimize num iteration value optimizable get value logger fine entering l f g optimize value +initial value g first logger fine first l f g iteration 0 link y link rho link alpha m 0 m i++ alpha 0 0 optimizable get num old optimizable get num g optimizable get num oldg optimizable get num direction optimizable get num optimizable get arraycopy 0 old 0 length optimizable get value gradient g arraycopy g 0 oldg 0 g length arraycopy g 0 direction 0 g length matrix ops ab normalize direction 0 logger info l f g gradient zero saying converge g converge logger fine direction 2norm + matrix ops two norm direction matrix ops time equal direction 1 0 matrix ops two norm direction jump logger fine jump direction 2norm + matrix ops two norm direction + gradient 2norm + matrix ops two norm g + 2norm + matrix ops two norm test maximizable test value gradient direction maxable direction step line maximiz optimize direction step step 0 0 could step direction give up say converge g reset search step 1 0 optimization line search could step current direction + alarm sometime happen close maximum + function very flat optimizable get optimizable get value gradient g logger fine jump direction 2norm + matrix ops two norm direction + gradient 2norm + matrix ops two norm g iteration count 0 iteration count num iteration iteration count++ value optimizable get value logger fine l f g iteration +iteration count + value +value+ g two norm + matrix ops two norm g + oldg two norm + matrix ops two norm oldg get difference between previou 2 gradient sy 0 0 yy 0 0 0 old length i++ inf inf 0 inf inf 0 infinite infinite old old 0 old 0 0 old old infinite g infinite oldg g oldg 0 oldg 0 0 oldg g oldg sy + old oldg si yi yy + oldg oldg direction g sy 0 optimizable sy +sy+ 0 gamma sy yy scaling factor gamma 0 optimizable gamma +gamma+ 0 push rho 1 0 sy these now difference between gradient push old push y oldg size y size size + size + y size + y size next we calculate direction first work backward most recent difference vector size 1 0 alpha rho get value matrix ops dot get direction matrix ops plus equal direction y get 1 0 alpha scale direction ratio y y y matrix ops time equal direction gamma now work forward old difference vector 0 y size i++ beta rho get value matrix ops dot y get direction matrix ops plus equal direction get alpha beta move current value last iteration buffer negate search direction 0 oldg length i++ old oldg g direction 1 0 logger fine linesearch direction gradient dotprod + matrix ops dot direction g + direction 2norm + matrix ops two norm direction + 2norm + matrix ops two norm test gradient ok test maximizable test value gradient direction maxable direction line search current direction step line maximiz optimize direction step step 0 0 could step direction g reset search step 1 0 xxx temporary test pass o k test maximizable test value gradient direction maxable direction optimization line search could step current direction + alarm sometime happen close maximum + function very flat optimizable get optimizable get value gradient g logger fine linesearch direction 2norm + matrix ops two norm direction value optimizable get value test termination 2 0 math ab value value tolerance math ab value + math ab value + ep logger info exit l f g termination 1 value difference below tolerance old value + value + value + value converge gg matrix ops two norm g gg gradient tolerance logger fine exit l f g termination 2 gradient +gg+ +gradient tolerance converge gg 0 0 logger fine exit l f g termination 3 gradient 0 0 converge logger fine gradient +gg iterations++ iteration max iteration err too many iteration l f g current converge illegal too many iteration iteration call evaluator eval eval evaluate optimizable iteration count logger fine exit l f g termination 4 evaluator converge reset previou gradient value approximate hessian n o t e link optimizable externally call illegal reset g push onto queue l l link queue matrix obj toadd matrix push onto queue push link l toadd l size m l size m old matrix add efficient actually overwrite memory old matrix overwrite old matrix last l get 0 arraycopy toadd 0 last 0 toadd length ptr last readjust pointer 0 l size 1 i++ l l get i+1 l m 1 ptr toadd length arraycopy toadd 0 0 toadd length l add last push onto queue l l link queue obj toadd value push onto queue push link l toadd l size m l size m pop old add l first l add last toadd l add last toadd 