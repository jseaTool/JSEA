io o io input stream io output stream random logging logger optimize memory f g optimize optimiz type gain type feature inducer type feature selection type feature vector type gradient gain type info gain type instance type instance type label type label alphabet type label type label vector type rank feature vector type logger unlike classifier trainer transducer trainer stateles between call train transducer trainer construct pair specific transducer can train transducer r f store feature selection weight freezing r f trainer store content dimension sparsity feature induction r f weight determine training p note future go away favor some r f trainer value gradient r f trainer label likelihood transducer trainer transducer trainer optimization logger logger logger get logger r f trainer label likelihood get name e f u l t g u n p r o r v r n e 1 0 e f u l t h y p e r o l p r o r l o p e 0 2 e f u l t h y p e r o l p r o r h r p n e 10 0 r f crf optimizable r f ocrf r f optimizable label likelihood ocrf optimiz opt iteration count 0 converge hyperbolic prior gaussian prior variance e f u l t g u n p r o r v r n e hyperbolic prior slope e f u l t h y p e r o l p r o r l o p e hyperbolic prior sharpness e f u l t h y p e r o l p r o r h r p n e sparse weight weight t o o just debug some unsupport trick variou value r f indicator we need cache value weight stamp 1 re calculate expectation value get value because weight value change cache gradient weight stamp 1 re calculate get value gradient because weight value change cache weight structure stamp 1 re allocate crf weight expectation constraint because transition mcrf training see we need re allocate crf weight expectation constraint because we different training last xxx temporary hack quite useful though ca print gradient r f trainer label likelihood r f crf crf crf transducer get transducer crf r f get r f crf optimiz get optimiz opt converge converge finish training converge get iteration iteration count specify factor r f trainer you already setup factor r f you want trainer add additional factor flag trainer add factor r f add factor flag weight flag r f optimizable label likelihood get optimizable r f instance training cache weight structure stamp crf weight structure change stamp weight sparse weight crf weight dimension training some unsupport trick crf weight dimension densely reallocate statistic here because constructor optimizable r f ocrf cache weight structure stamp crf weight structure change stamp ocrf || ocrf training training ocrf optimizable r f crf training ocrf r f optimizable label likelihood crf training ocrf gaussian prior variance gaussian prior variance ocrf hyperbolic prior sharpness hyperbolic prior sharpness ocrf hyperbolic prior slope hyperbolic prior slope ocrf hyperbolic prior hyperbolic prior opt ocrf optimiz get optimiz instance training get optimizable r f training mcrf opt || ocrf opt get optimizable opt memory f g ocrf alternative opt conjugate gradient 0 001 opt question inner r f trainer can subclass another can subclass still access r f instance variables? n w e r yes yes you syntax subclass ctor see dev archive ca train incremental instance training train training m v l u e train instance training num iteration num iteration 0 training size 0 get optimizable r f training mcrf get optimiz training opt converge logger info r f about train +num iterations+ iteration 0 num iteration i++ converge opt optimize 1 iteration count++ logger info r f finish iteration maximiz +i run evaluator illegal argument e e print stack trace logger info catching saying converge converge e e print stack trace logger info catching saying converge converge converge logger info r f training converge +i converge train r f variou size subset typically accelerate training quickly getting subset first progressively training training instance num iteration proportion maximum maximiz iteration training proportion training proportion train increasingly large portion e g 0 2 0 5 1 0 can sometime speedup convergence sure 1 0 you want train training converge train instance training num iteration proportion training proportion training iteration 0 training proportion length 0 converge 0 training proportion length i++ training proportion 1 0 logger info training +train proportion + % round training proportion 1 0 converge train training num iteration proportion converge train training split random 1 training proportion 1 training proportion 0 num iteration proportion training iteration + num iteration proportion converge train feature induction instance training instance validation instance testing transducer evaluator eval num iteration num iteration between feature induction num feature induction num feature feature induction label prob threshold cluster feature induction training proportion train feature induction training validation testing eval num iteration num iteration between feature induction num feature induction num feature feature induction label prob threshold cluster feature induction training proportion train r f feature induction generate conjunction feature feature induction run periodically dur training feature improve mislabel instance specific scoring criterion link feature inducer specify gain name training training instance validation validation instance testing testing instance eval evaluation dur training num iteration maximum maximiz iteration num iteration between feature induction maximiz iteration between call feature inducer num feature induction maximum round feature induction num feature feature induction maximum feature induce round induction label prob threshold model probability label instance les value instance link feature inducer cluster feature induction link feature inducer construct label pair can inducing disproportionate feature single label training proportion train increasingly large portion e g 0 2 0 5 1 0 can sometime speedup convergence gain name type link feature inducer grad info link gain link gradient gain link info gain training converge train feature induction instance training instance validation instance testing transducer evaluator eval num iteration num iteration between feature induction num feature induction num feature feature induction label prob threshold cluster feature induction training proportion gain name training iteration 0 num label crf output alphabet size crf global feature selection training get feature selection crf global feature selection mask feature some late feature inducer induce feature crf global feature selection feature selection training get alphabet training feature selection crf global feature selection t o o careful validation testing get remove argument next two line work somewhere validation validation feature selection crf global feature selection testing testing feature selection crf global feature selection feature induction iteration 0 feature induction iteration num feature induction feature induction iteration++ print some feature information logger info feature induction iteration +feature induction iteration train r f instance training training training proportion feature induction iteration training proportion length logger info training +train proportion feature induction iteration + % round instance sample training training split random 1 training proportion feature induction iteration 1 training proportion feature induction iteration training sample training 0 training feature selection crf global feature selection xxx necessary? logger info +the training size + instance converge feature induction iteration 0 t train until we some feature converge train training num iteration between feature induction training iteration + num iteration between feature induction logger info starting feature induction +crf input alphabet size + feature both uncluster cluster feature induction instance instance instance training get alphabet training get target alphabet instance feature selection get examine feature inducer can know add singleton feature instance feature selection crf global feature selection label vector instance cluster instance instance num label num label cluster label vector num label num label 0 num label i++ j 0 j num label j++ cluster instance j instance training get alphabet training get target alphabet cluster instance j feature selection crf global feature selection cluster label vector j 0 training size i++ logger info instance +i instance instance training get input instance get output instance get target input size output size sum lattice lattice crf sum lattice factory sum lattice crf input transducer incrementor label alphabet training get target alphabet prev label index 0 put extra instance cluster j 0 j output size j++ label label label label output get label position j label instance +i+ position +j+ fv +lattice get label position j label vector lattice label lattice get label position j label prob lattice label value label get index label index lattice label get best index position +j+ label prob +true label prob label prob label prob threshold logger info add instance +i+ position +j+ prtrue +true label prob+ label lattice label get best label ? + truelabel +label+ predlabel +lattice label get best label + fv + feature vector input get j instance add input get j label label vector add lattice label cluster instance prev label index label index add input get j label cluster label vector prev label index label index add lattice label prev label index label index logger info instance size +error instance size cluster feature induction feature inducer klfi feature inducer num label num label 0 num label i++ j 0 j num label j++ note we see some impossible transition here o o model because we lattice gamma get predict label viterbi t believe harm some good logger info feature induction + crf output alphabet lookup + +crf output alphabet lookup j + +cluster instance j size + instance cluster instance j size 20 logger info skip because +cluster instance j size + instance cluster label vector j size label vector lv label vector k 0 k k++ lv k label vector cluster label vector j get k rank feature vector factory gain factory gain name equal gain factory gain factory lv gaussian prior variance gain name equal grad gain factory gradient gain factory lv gain name equal info gain factory info gain factory klfi j feature inducer gain factory cluster instance j num feature feature induction 2 num feature feature induction 2 num feature feature induction crf feature inducer add klfi j 0 num label i++ j 0 j num label j++ logger info add induce feature + crf output alphabet lookup + +crf output alphabet lookup j klfi j logger info skip because feature induce note add feature globally transition klfi j induce feature training testing klfi j induce feature testing klfi label vector size label vector lv label vector 0 i++ lv label vector label vector get rank feature vector factory gain factory gain name equal gain factory gain factory lv gaussian prior variance gain name equal grad gain factory gradient gain factory lv gain name equal info gain factory info gain factory feature inducer klfi feature inducer gain factory instance num feature feature induction 2 num feature feature induction 2 num feature feature induction crf feature inducer add klfi note add feature globally transition klfi induce feature training testing klfi induce feature testing logger info r f4 feature selection now +crf global feature selection cardinality + feature klfi r f4 train anyway weight dimension training grow weight dimension input alphabet train training num iteration training iteration hyperbolic prior f hyperbolic prior f hyperbolic prior slope p hyperbolic prior slope p hyperbolic prior sharpness p hyperbolic prior sharpness p get hyperbolic prior slope hyperbolic prior slope get hyperbolic prior sharpness hyperbolic prior sharpness gaussian prior variance p gaussian prior variance p get gaussian prior variance gaussian prior variance get feature index feature index sparse weight sparse weight get sparse weight sparse weight set some unsupport trick trick training r f some training be sparse weight add few weight feaure occur tainig p generally better accuracy small memory cost trick some unsupport trick some unsupport trick serialization r f trainer likelihood serial u 1 u r r e n t e r l v e r o n 1 n u l l n t e g e r 1 need check pointer write output stream o size write u r r e n t e r l v e r o n write feature index write hyperbolic prior write gaussian prior variance write hyperbolic prior slope write hyperbolic prior sharpness write cache gradient weight stamp write cache value weight stamp write cache weight structure stamp write print gradient write sparse weight illegal yet complete read input stream o found size read feature index read hyperbolic prior read gaussian prior variance read hyperbolic prior slope read hyperbolic prior sharpness read print gradient read sparse weight read illegal yet complete 