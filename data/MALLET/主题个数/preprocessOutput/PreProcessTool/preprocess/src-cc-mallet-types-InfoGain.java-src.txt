2002 univ dept part m l l e t m achine learning languag e toolkit 1 0 information see ` l e n e information gain absence precense feature note we aren t attending feature value m l l e t doesn t currently support categorical feature mc callum type info gain rank feature vector xxx g u t n g l y thread safe base entropy label vector base label xxx yuck figure strictly part feature info gain convenient efficient ml classify decision tree base entropy label vector base label calc info gain instance ilist log2 math log 2 num instance ilist size num ilist get target alphabet size num feature ilist get alphabet size infogain num feature target feature count num num feature feature count sum num feature target count num target count sum 0 flv feature location value fli feature location index count populate target feature count et al 0 ilist size i++ instance inst ilist get label label inst get label feature vector fv feature vector inst get instance weight ilist get instance weight below relie label weight sum 1 over label label weight sum 0 ll 0 ll label num location ll++ label index location ll label weight label value location ll label weight sum + label weight label weight 0 count label weight instance weight fl 0 fl fv num location fl++ fli fv index location fl xxx right? what we about negative values? whatever decide here also go decision tree split fv value location fl 0 target feature count fli + count feature count sum fli + count target count + count target count sum + count math ab label weight sum 1 0 0 0001 target count sum 0 base entropy 0 0 xxx instead infinite? base label label vector label alphabet ilist get target alphabet target count infogain target count sum 0 target count sum p num calculate overall entropy label ignore feature base entropy 0 print target count vector print target count target count sum +target count sum 0 num li++ p target count target count sum p p 1 0 p p 0 base entropy p math log p log2 base label label vector label alphabet ilist get target alphabet total entropy +static base entropy calculate info gain feature fi 0 fi num feature fi++ feature present entropy 0 norm feature count sum fi norm 0 0 num li++ p target feature count fi norm p 1 00000001 p p 0 feature present entropy p math log p log2 na n feature present entropy fi norm target count sum feature count sum fi feature absent entropy 0 norm 0 0 num li++ p target count target feature count fi norm p 1 00000001 p p 0 feature absent entropy p math log p log2 na n feature absent entropy fi alphabet dictionary ilist get alphabet feature +dictionary lookup symbol fi + present weight + feature count sum fi target count sum + absent weight + target count sum feature count sum fi target count sum + present entropy +feature present entropy+ absent entropy +feature absent entropy infogain fi base entropy feature count sum fi target count sum feature present entropy target count sum feature count sum fi target count sum feature absent entropy na n infogain fi fi infogain info gain instance ilist ilist get alphabet calc info gain ilist base entropy base entropy base label base label info gain alphabet vocab infogain vocab infogain get base entropy base entropy label vector get base label base label factory rank feature vector factory factory rank feature vector rank feature vector instance ilist info gain ilist 