random logging logger optimize memory f g optimize optimiz type instance logger gregory druck gdruck multi thread r f trainer note multi thread feature induction hyperbolic prior support r f trainer thread label likelihood transducer trainer transducer trainer optimization logger logger logger get logger r f trainer thread label likelihood get name e f u l t g u n p r o r v r n e 1 0 sparse weight weight some unsupport trick converge num thread iteration count gaussian prior variance r f crf r f optimizable batch label likelihood optimizable thread optimizable thread optimizable optimiz optimiz cache weight structure stamp r f trainer thread label likelihood r f crf num thread crf crf sparse weight weight some unsupport trick converge num thread num thread iteration count 0 gaussian prior variance e f u l t g u n p r o r v r n e cache weight structure stamp 1 transducer get transducer crf r f get r f crf optimiz get optimiz optimiz converge converge finish training converge get iteration iteration count gaussian prior variance p gaussian prior variance p get gaussian prior variance gaussian prior variance sparse weight sparse weight get sparse weight sparse weight set some unsupport trick trick training r f some training be sparse weight add few weight feaure occur tainig p generally better accuracy small memory cost trick some unsupport trick some unsupport trick specify factor r f trainer you already setup factor r f you want trainer add additional factor flag trainer add factor r f add factor flag weight flag shutdown thread optimizable shutdown r f optimizable batch label likelihood get optimizable r f instance training cache weight structure stamp crf weight structure change stamp weight sparse weight crf weight dimension training some unsupport trick crf weight dimension densely optimizable cache weight structure stamp crf weight structure change stamp optimizable || optimizable training training optimizable r f optimizable batch label likelihood crf training num thread optimizable gaussian prior variance gaussian prior variance thread optimizable thread optimizable optimizable training crf get get num factor r f cache stale indicator crf optimiz optimizable optimiz get optimiz instance training get optimizable r f training optimiz || optimizable optimiz get optimizable optimiz memory f g thread optimizable optimiz train incremental instance training train training m v l u e train instance training num iteration num iteration 0 training size 0 get optimizable r f training mcrf get optimiz training opt converge logger info r f about train +num iterations+ iteration 0 num iteration i++ converge optimiz optimize 1 iteration count++ logger info r f finish iteration maximiz +i run evaluator illegal argument e e print stack trace logger info catching saying converge converge e e print stack trace logger info catching saying converge converge converge logger info r f training converge +i converge train r f variou size subset typically accelerate training quickly getting subset first progressively training training instance num iteration proportion maximum maximiz iteration training proportion training proportion train increasingly large portion e g 0 2 0 5 1 0 can sometime speedup convergence sure 1 0 you want train training converge train instance training num iteration proportion training proportion training iteration 0 training proportion length 0 converge 0 training proportion length i++ training proportion 1 0 logger info training +train proportion + % round training proportion 1 0 converge train training num iteration proportion converge train training split random 1 training proportion 1 training proportion 0 num iteration proportion training iteration + num iteration proportion converge 