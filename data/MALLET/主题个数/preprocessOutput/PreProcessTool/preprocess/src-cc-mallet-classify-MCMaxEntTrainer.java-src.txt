2002 univ dept part m l l e t m achine learning languag e toolkit 1 0 furtherinformation see ` l e n e classify logging io classify classifier optimize memory f g optimize optimizable optimize optimiz optimize test pipe pipe type alphabet type gain type feature inducer type feature selection type feature vector type gradient gain type info gain type instance type instance type label type label alphabet type label vector type label type matrix ops type rank feature vector type vector command logger progress message logger math currently handle instance label distribution instead single label trainer maximum entropy classifier mc callum m max ent trainer classifier trainer m max ent boostable serializable command provide logger logger logger get logger m max ent trainer get name logger progress logger progress message logger get logger m max ent trainer get name + pl num get value call 0 num get value gradient call 0 num iteration 10 e p g n g r e n t g n grad n f o r m t o n g n info xxx why test maximizable fail variance very small? e f u l t g u n p r o r v r n e 1 note 1 e f u l t h y p e r o l p r o r l o p e 0 2 e f u l t h y p e r o l p r o r h r p n e 10 0 e f u l t m m z e r l memory f g p l multi conditional training hyperbolic prior gaussian prior variance e f u l t g u n p r o r v r n e hyperbolic prior slope e f u l t h y p e r o l p r o r l o p e hyperbolic prior sharpness e f u l t h y p e r o l p r o r h r p n e maximiz e f u l t m m z e r l generative weighting 1 0 maximizable trainer mt m max ent classifier p l command multi conditional training command m max ent trainer m training true|false multi conditional training command hyperbolic prior command m max ent trainer hyperbolic prior true|false hyperbolic close l1 penalty prior over command gaussian prior variance command m max ent trainer gaussian prior variance f l o t 10 0 variance gaussian prior over command hyperbolic prior slope command m max ent trainer hyperbolic prior slope f l o t 0 2 slope l1 penalty hyperbolic prior over command hyperbolic prior sharpness command m max ent trainer hyperbolic prior sharpness f l o t 10 0 sharpness l1 penalty hyperbolic prior over command command option command m maximum entropy classifier command hyperbolic prior gaussian prior variance hyperbolic prior slope hyperbolic prior sharpness multi conditional training p l command get command command option m max ent trainer maximiz gradient maximiz maximiz gradient maximiz hyperbolic prior m max ent trainer command col hyperbolic prior hyperbolic prior value gaussian prior variance gaussian prior variance value hyperbolic prior slope hyperbolic prior slope value hyperbolic prior sharpness hyperbolic prior sharpness value multi conditional training multi conditional training value m max ent trainer m max ent classifier classifier classifier m max ent trainer m max ent trainer hyperbolic prior hyperbolic prior hyperbolic prior construct trainer parameter overtrain 1 0 usually value m max ent trainer gaussian prior variance hyperbolic prior gaussian prior variance gaussian prior variance p l multi conditional training m max ent trainer gaussian prior variance multi conditional training hyperbolic prior multi conditional training multi conditional training gaussian prior variance gaussian prior variance m max ent trainer hyperbolic prior slope hyperbolic prior sharpness hyperbolic prior hyperbolic prior slope hyperbolic prior slope hyperbolic prior sharpness hyperbolic prior sharpness optimizable gradient value get maximizable trainer instance ilist ilist maximizable trainer maximizable trainer ilist specifie maximum iteration run dur single call train train feature induction currently functional trainer since we maximize num iteration doesn t work bug? num iteration higher? m max ent trainer num iteration num iteration m max ent trainer hyperbolic prior hyperbolic prior hyperbolic prior hyperbolic prior set parameter prevent overtrain small variance prior feature weight expect hover closer 0 extra evidence high weight trainer m max ent trainer gaussian prior variance gaussian prior variance hyperbolic prior gaussian prior variance gaussian prior variance m max ent trainer hyperbolic prior slope hyperbolic prior slope hyperbolic prior hyperbolic prior slope hyperbolic prior slope m max ent trainer hyperbolic prior sharpness hyperbolic prior sharpness hyperbolic prior hyperbolic prior sharpness hyperbolic prior sharpness m max ent get classifier mt get classifier m max ent train instance training logger fine training size +train size mt maximizable trainer training m max ent classifier optimiz maximiz memory f g mt p l change tolerance large vocab experiment memory f g maximiz tolerance 00001 std 0001 maximiz optimize loop below seem wrong logger info m max ent nget value call +get value call + m max ent nget value gradient call +get value gradient call converge 0 num iteration i++ converge maximiz maximize mt 1 converge evaluator evaluator evaluate mt get classifier converge mt get value training validation test test maximizable test value gradient mt progress logger info proges message line move mt get classifier p train feature induction some option change p maxent partially train classifier classifier dur training gain name estimate gain log likelihood increase we want our chosen feature maximize max ent trainer e p g n max ent trainer g r e n t g n max ent trainer n f o r m t o n g n e p g n train max ent classifier classifier train feature induction instance training instance validation instance testing classifier evaluate evaluator m max ent maxent total iteration num iteration between feature induction num feature induction num feature feature induction gain name ought parameter setting can crash training jump too small save dur f alphabet input alphabet training get alphabet alphabet output alphabet training get target alphabet maxent maxent m max ent training get pipe 1+input alphabet size output alphabet size training iteration 0 num label output alphabet size initialize feature selection feature selection global f training get feature selection global f mask feature some late feature inducer induce feature global f feature selection training get alphabet training feature selection global f validation validation feature selection global f testing testing feature selection global f maxent m max ent maxent get instance pipe maxent get global f run feature induction feature induction iteration 0 feature induction iteration num feature induction feature induction iteration++ print some feature information logger info feature induction iteration +feature induction iteration train model little bit we t care converge we execute feature induction iteration matter what feature induction iteration 0 t train until we some feature num iteration num iteration between feature induction maxent m max ent train training validation testing evaluator maxent training iteration + num iteration between feature induction logger info starting feature induction + 1+input alphabet size + feature over +num labels+ label instance instance instance training get alphabet training get target alphabet instance feature selection get examine feature inducer can know add singleton feature instance feature selection global f label vector these length 1 vector 0 training size i++ instance instance training get feature vector input vector feature vector instance get label label label instance get target have train just current feature see we classify training now classification classification maxent classify instance classification best label correct instance add input vector label label vector add classification get label vector logger info instance size +error instance size label vector size label vector lv label vector 0 i++ lv label vector label vector get rank feature vector factory gain factory gain name equal e p g n gain factory gain factory lv gaussian prior variance gain name equal g r e n t g n gain factory gradient gain factory lv gain name equal n f o r m t o n g n gain factory info gain factory illegal argument unsupport gain name +gain name feature inducer klfi feature inducer gain factory instance num feature feature induction 2 num feature feature induction 2 num feature feature induction note add feature globally transition klfi induce feature training testing klfi induce feature testing logger info m max ent feature selection now +global f cardinality + feature klfi 1+input alphabet size output alphabet size executing block often dur training t know why save dur f keep current parameter value relie detail most recent feature alphabet get high index count output label old count maxent length output alphabet size count 1+input alphabet size into proper location 0 output alphabet size i++ arraycopy maxent old count count old count 0 old count i++ maxent maxent + +new exit 0 maxent maxent feature index input alphabet size finish feature induction logger info end +global f cardinality + feature num iteration total iteration training iteration train training validation testing evaluator maxent these really public? why? count many time trainer compute gradient log probability training label get value gradient call num get value gradient call count many time trainer compute log probability training label get value call num get value call get iteration maximiz gradient get iteration m max ent trainer + +maximiz get name + + num iteration + num iteration + hyperbolic prior ? hyperbolic prior slope +hyperbolic prior slope+ hyperbolic prior sharpness +hyperbolic prior sharpness gaussian prior variance +gaussian prior variance inner wrap up m max ent classifier training maximize maximizable function maximizable trainer optimizable gradient value constraint cache gradient m max ent classifier instance training expectation temporarily store cache gradient cache value cache value stale cache gradient stale num label num feature feature index just clarity feature selection feature selection feature selection label feature selection maximizable trainer maximizable trainer instance ilist m max ent classifier training ilist alphabet fd ilist get alphabet label alphabet ld label alphabet ilist get target alphabet t fd stop growth because someone want feature induction ld stop growth add feature feature num label ld size num feature fd size + 1 feature index num feature 1 num label num feature constraint num label num feature cache gradient num label num feature fill 0 0 fill constraint 0 0 fill cache gradient 0 0 feature selection ilist get feature selection label feature selection ilist get label feature selection add feature index selection feature selection feature selection add feature index label feature selection 0 label feature selection length i++ label feature selection add feature index xxx late change both select flag? feature selection || label feature selection classifier classifier classifier classifier feature selection classifier feature selection label feature selection classifier feature selection feature index classifier feature index classifier get instance pipe ilist get pipe classifier classifier m max ent ilist get pipe feature selection label feature selection cache value stale cache gradient stale initialize constraint logger fine instance training + training size instance inst training instance weight training get instance weight inst label label inst get label logger fine instance +ii+ label +label feature vector fv feature vector inst get alphabet fdict fv get alphabet fv get alphabet fd label get best index 2 below because there p y|x another p x|y matrix ops row plus equal constraint num feature fv 2 instance weight feature weight 1 0 na n instance weight instance weight na n na n best index na n na n 0 fv num location i++ na n fv value location logger info na n feature + fdict lookup fv index location na n na n logger info na n instance + inst get name p y|x us feature p x|y doesn t feature value 1 0 constraint num feature + feature index + instance weight test maximizable test value gradient current m max ent get classifier classifier get parameter index index parameter index v cache value stale cache gradient stale index v get num length get buff buff || buff length length buff length arraycopy 0 buff 0 length buff buff cache value stale cache gradient stale buff length length buff length arraycopy buff 0 0 buff length log probability training label get value cache value stale num get value calls++ cache value 0 we ll store expectation value cache gradient now cache gradient stale fill cache gradient 0 0 incorporate likelihood score training get target alphabet size value 0 0 now +input alphabet size + regular feature iterator instance it training iterator 0 normalize multinomial prob score length num feature lprob score length num feature si 0 si score length si++ sum 0 max matrix ops max fi 0 fi num feature fi++ t o o strongly consider some smooth here what happen zero? oh problem because 0 1 prob si fi math si num features+fi max sum + prob si fi sum 0 fi 0 fi num feature fi++ prob si fi sum lprob si fi math log prob si fi it next instance instance it next instance weight training get instance weight instance label label instance get label l now +input alphabet size + regular feature classifier get classification score instance score feature vector fv feature vector instance get label get best index value instance weight math log score na n value logger fine m max ent trainer instance + instance get name + na n value log score + math log score + score + score + instance weight + instance weight infinite value logger warning instance +instance get + infinite value skip value gradient cache value value cache value stale value cache value + value p l loop over score we compute gradient taking dot feature value probability si 0 si score length si++ score si 0 infinite score si p l accumulate current classifier expectation feature vector count label current classifier expectation over label over feature vector matrix ops row plus equal cache gradient num feature si fv instance weight score si cache gradient num feature si + feature index + instance weight score si p l we wish multiconditional training we need another term accumulate expectation multi conditional training need someth analogou classifier get classification score instance score classifier get feature distribution instance note label instance get sum feature vector count document we input ncount matrix ops sum fv p l get additional term value our log probability computation amount dot feature vector probability vector cache value instance weight fv dot lprob p l get model expectation over feature fi 0 fi num feature fi++ num feature + fi 0 matrix ops row plus equal cache gradient num feature fv cache gradient num feature + fi + instance weight ncount prob fi logger info expectation cache gradient print incorporate prior hyperbolic prior 0 num label li++ fi 0 fi num feature fi++ cache value + hyperbolic prior slope hyperbolic prior sharpness math log math cosh hyperbolic prior sharpness num feature + fi 0 num label li++ fi 0 fi num feature fi++ num feature + fi cache value + 2 gaussian prior variance cache value 1 0 m m z e n o t m n m z e cache value stale progress logger info value loglikelihood +cach value cache value p l first get value gradient get value gradient buffer gradient constraint expectation gaussian prior variance cache gradient stale num get value gradient calls++ cache value stale fill cache gradient expectation get value cache gradient contain negative expectation expectation model expectation constraint empirical expectation matrix ops plus equal cache gradient constraint p l we need second constraint actually we want feature value ve move up into get value multi conditional training matrix ops plus equal cache gradient constraint incorporate prior hyperbolic prior unsupport operation hyperbolic prior yet matrix ops plus equal cache gradient 1 0 gaussian prior variance parameter infinity external user we gradient 0 because parameter value can nev change anyway mess up future calculation matrix norm matrix ops substitute cache gradient n e g t v e n f n t y 0 0 zero gradient dimension among select feature label feature selection label index 0 label index num label label index++ matrix ops row cache gradient num feature label index 0 0 feature selection label index 0 label index num label label index++ matrix ops row cache gradient num feature label index 0 0 label feature selection label index cache gradient stale buffer buffer length length arraycopy cache gradient 0 buffer 0 cache gradient length sum neg log prob p o t v e n f n t y p o t v e n f n t y p o t v e n f n t y math log 1 + math math log 1 + math 