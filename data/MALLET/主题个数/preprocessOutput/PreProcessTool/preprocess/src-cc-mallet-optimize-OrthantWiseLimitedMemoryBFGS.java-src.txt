optimize link logging logger type matrix ops logger orthant wise memory quasi newton optimize convex l1 regularize objective see scalable training l1 regularize log linear model galen jianfeng gao m l 2007 detail adaptation freely c++ galen webpage kedar bellare orthant wise memory f g optimiz logger logger logger get logger orthant wise memory f g get name converge optimizable gradient value optimizable name optimizable value output opt name max iteration 1000 tolerance 0001 gradient tolerance 001 ep 1 0e 5 l1 weight correction f g update ideally 3 m 7 large m cpu memory m 4 optimiz search old value value line search value value line search old value value y dot y grad gradient grad old grad direction steep descent direction old m previou difference value y m previou difference grad value link y rho intermediate calculation link rho alpha iteration orthant wise memory f g optimizable gradient value function function 0 0 orthant wise memory f g optimizable gradient value function l1wt optimizable function l1 weight l1wt part optimizable get get name split \\ opt name part part length 1 initialize optimiz iteration 0 link y link rho link alpha m matrix ops alpha 0 0 y dot y 0 num optimizable get num get num optimizable get get value value eval l1 get gradient grad num eval gradient initialize direction direction num steep descent direction num initialize backup old num old grad num optimizable get optimizable optimizable converge converge get iteration iteration optimize optimize m v l u e optimize num iteration logger fine entering o w l f g optimize l1 weight + l1 weight + value + value it 0 it num iteration iter++ descent direction steep desc dir adjust curvature map dir inverse hessian y dot y fix direction sign fix dir sign backup gradient line search store src d old store src d grad old grad back tracking line search update gradient line search eval gradient check termination check value termination logger info exit o w l f g termination 1 logger info value difference below tolerance old value + old value + value + value converge check gradient termination logger info exit o w l f g termination 2 logger info gradient + matrix ops two norm grad + + gradient tolerance converge update hessian approximation y dot y shift iterations++ iteration max iteration logger info too many iteration o w l f g + current converge evaluate value minimization problem eval l1 val optimizable get value sum ab wt 0 l1 weight 0 infinite sum ab wt + math ab l1 weight logger info get value + opt name + get value + val + + |w| + sum ab wt + + val + sum ab wt val + sum ab wt evaluate gradient descent direction eval gradient optimizable get value gradient grad adjust grad infinite grad matrix ops time equal grad 1 0 create steep ascent direction gradient l1 regularization steep desc dir l1 weight 0 0 grad length i++ direction grad 0 grad length i++ 0 direction grad + l1 weight 0 direction grad l1 weight grad l1 weight direction grad l1 weight grad l1 weight direction grad + l1 weight direction 0 store src d direction steep descent direction adjust grad infinite 0 length i++ infinite 0 adjust direction approximate hessian inverse y dot y y^ t y f g calculation map dir inverse hessian y dot y size 0 count size count 1 0 alpha matrix ops dot get direction rho get matrix ops plus equal direction y get alpha scalar rho get count 1 y dot y logger fine direction multiplier + scalar matrix ops time equal direction scalar 0 count i++ beta matrix ops dot y get direction rho get matrix ops plus equal direction get alpha beta fix dir sign l1 weight 0 0 direction length i++ direction steep descent direction 0 direction 0 dir deriv l1 weight 0 matrix ops dot direction grad val 0 0 0 direction length i++ direction 0 0 val + direction grad l1 weight 0 val + direction grad + l1 weight direction 0 val + direction grad l1 weight direction 0 val + direction grad + l1 weight val shift next next y size size size m next length next y length next first next y y first rho first rho 0 0 y dot y 0 0 0 length i++ infinite infinite old old 0 next 0 next old infinite grad infinite old grad grad old grad 0 next y 0 next y grad old grad rho + next next y y dot y + next y next y logger fine rho + rho rho 0 optimizable rho + rho + 0 + hessian inverse + gradient change opposite parameter change add last next y add last next y rho add last rho update old grad store src d old store src d grad old grad y dot y store src d src d arraycopy src 0 d 0 src length backtrack line search back tracking line search orig dir deriv dir deriv orig dir deriv 0 optimizable l f g choose ascent direction check gradient alpha 1 0 backoff 0 5 iteration 0 norm dir math sqrt matrix ops dot direction direction alpha 1 0 norm dir backoff 0 1 c1 1e 4 store old value old value value logger fine starting line search it + iteration logger fine it + iteration + value start line search + value update gradient get next point alpha find value value eval l1 logger fine it + iteration + alpha + alpha + value + value + |grad| + matrix ops two norm grad + |x| + matrix ops two norm value old value + c1 orig dir deriv alpha alpha backoff get next point alpha 0 length i++ old + direction alpha l1 weight 0 orthant boundary l1 regularization old 0 0 0 optimizable termination check value termination 2 0 math ab value old value tolerance math ab value + math ab old value + ep check gradient termination matrix ops two norm grad gradient tolerance 